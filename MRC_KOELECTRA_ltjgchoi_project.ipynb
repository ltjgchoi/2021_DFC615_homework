{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRC_KOELECTRA_ltjgchoi_project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6dc1030b3cab4fd0a0a1cf527675c3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9037fd5a4840411d8fc1fe8ce05f5818",
              "IPY_MODEL_accdbc37b53845beb77223c5ede78f47"
            ],
            "layout": "IPY_MODEL_11afcdadad37455185390d1785e24961"
          }
        },
        "9037fd5a4840411d8fc1fe8ce05f5818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3637e68b16804f34a951c1bab36c395c",
            "max": 458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_884054df11f54a4582bdb618ee7d17ae",
            "value": 458
          }
        },
        "accdbc37b53845beb77223c5ede78f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb9c69c2d8a24aa0ac24ed634f6f4232",
            "placeholder": "​",
            "style": "IPY_MODEL_6369d806d4c249b3bcc00de3ee0b4cfc",
            "value": " 458/458 [00:00&lt;00:00, 1.00kB/s]"
          }
        },
        "11afcdadad37455185390d1785e24961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3637e68b16804f34a951c1bab36c395c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884054df11f54a4582bdb618ee7d17ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "fb9c69c2d8a24aa0ac24ed634f6f4232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6369d806d4c249b3bcc00de3ee0b4cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef4d610738824273b044f85dc667d6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_904bc6ce8b8041338c7baae3e103c62b",
              "IPY_MODEL_499cd19011414d4bb7c5b66a0468dd40"
            ],
            "layout": "IPY_MODEL_695abbba60044a2db36ae29f4c25c4dc"
          }
        },
        "904bc6ce8b8041338c7baae3e103c62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7c41177be44418811094367b6bc65c",
            "max": 263326,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc050a2c905f4a87a9d734c4adb84fd6",
            "value": 263326
          }
        },
        "499cd19011414d4bb7c5b66a0468dd40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0cb7bc6ea464b0ca077c53200061481",
            "placeholder": "​",
            "style": "IPY_MODEL_c7205ad5fdb642a1bbf2367879017d7d",
            "value": " 263k/263k [00:00&lt;00:00, 1.19MB/s]"
          }
        },
        "695abbba60044a2db36ae29f4c25c4dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7c41177be44418811094367b6bc65c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc050a2c905f4a87a9d734c4adb84fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d0cb7bc6ea464b0ca077c53200061481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7205ad5fdb642a1bbf2367879017d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e9c9bb9b0d400dad005c7dc0eab50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee562a8dd4d14ba79f153675ff21e1de",
              "IPY_MODEL_332b046a6e724a4596a44e55567fa681"
            ],
            "layout": "IPY_MODEL_a3b013b9ac8046e9ac62919ad21989f6"
          }
        },
        "ee562a8dd4d14ba79f153675ff21e1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cac23edcb2047e68a522ce0944596e7",
            "max": 61,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd7750d3b14c4466a02246bb9b6191b9",
            "value": 61
          }
        },
        "332b046a6e724a4596a44e55567fa681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1bd70862f464526b6bb343a7c9e5dfb",
            "placeholder": "​",
            "style": "IPY_MODEL_50a054c4300d407ca06769890fe8189c",
            "value": " 61.0/61.0 [00:00&lt;00:00, 551B/s]"
          }
        },
        "a3b013b9ac8046e9ac62919ad21989f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cac23edcb2047e68a522ce0944596e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7750d3b14c4466a02246bb9b6191b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d1bd70862f464526b6bb343a7c9e5dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a054c4300d407ca06769890fe8189c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dddce1d73034c83b05d4cedfd3b6b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_034780d6bd934665adc4978445683616",
              "IPY_MODEL_8ed9f01817ec477d96e6f539cf7f00bc"
            ],
            "layout": "IPY_MODEL_f1edea9c6210461780e5437b7774ede1"
          }
        },
        "034780d6bd934665adc4978445683616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c8f830a50b4c5590c4efbfc1375a13",
            "max": 56577499,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2ac93b9f0784f06ba3ebe1a9227b832",
            "value": 56577499
          }
        },
        "8ed9f01817ec477d96e6f539cf7f00bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a534c702bc546cf9684397de106ed10",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb7fc437e614aabb803396b6eec064d",
            "value": " 56.6M/56.6M [00:01&lt;00:00, 37.8MB/s]"
          }
        },
        "f1edea9c6210461780e5437b7774ede1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c8f830a50b4c5590c4efbfc1375a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ac93b9f0784f06ba3ebe1a9227b832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4a534c702bc546cf9684397de106ed10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb7fc437e614aabb803396b6eec064d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RakqjO7jiXYX"
      },
      "source": [
        "# 실습 7강. MRC (실습)\n",
        "\n",
        "본 실습 자료는 **DFC615(00) 인공지능과자연언어처리기술 강의 실습**을 위해 **고려대학교 자연어처리연구실 (NLP & AI Lab)**에서 제작했습니다.\n",
        "\n",
        "☠️ 학습 이외의 목적으로 무단 배포를 금지합니다. ☠️\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "version 1.0 (2021.06.03)\n",
        "created by: 박성진, 임희석 (고려대학교 자연어처리 연구실)\n",
        "email: ec_park@korea.ac.kr\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz9HgHTpid2o"
      },
      "source": [
        "## **시작하기 전에 반드시 해야할 것들** 😁\n",
        "\n",
        "\n",
        "ipynb 파일은 파이썬 언어를 기반으로 제작된 노트 파일이며, Jupyter Notebook과 Google Colab 상에서 읽기/쓰기가 가능합니다.\n",
        "\n",
        "➀ 크롬 다운로드와 구글 계정을 생성.   \n",
        "➁ 좌측 상단 '파일' -> '드라이브에 사본 저장...'   \n",
        "➂ 본인의 구글 계정 Gdrive에 접속해주세요  \n",
        "➃ 자동으로 'Colab Notebooks'라는 폴더가 만들어져 있습니다  \n",
        "(*없다면 새로고침를 눌러주세요)  \n",
        "➄ 해당 폴더 속에 사본 파일이 복사된 것을 확인   \n",
        "➅ 'Colab Notebooks'폴더 안에 꼭 'DFC615'라는 이름의 폴더를 만들어주세요  \n",
        "➆ 사본 ipynb 파일을 'DFC615' 폴더로 이동 시킨 후 열어주세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a-5HMsoibRr"
      },
      "source": [
        "## **시작하기 전에 반드시 해야할 것들** 😁\n",
        "\n",
        "\n",
        "ipynb 파일은 파이썬 언어를 기반으로 제작된 노트 파일이며, Jupyter Notebook과 Google Colab 상에서 읽기/쓰기가 가능합니다.\n",
        "\n",
        "➀ 크롬 다운로드와 구글 계정을 생성.   \n",
        "➁ 좌측 상단 '파일' -> '드라이브에 사본 저장...'   \n",
        "➂ 본인의 구글 계정 Gdrive에 접속해주세요  \n",
        "➃ 자동으로 'Colab Notebooks'라는 폴더가 만들어져 있습니다  \n",
        "(*없다면 새로고침를 눌러주세요)  \n",
        "➄ 해당 폴더 속에 사본 파일이 복사된 것을 확인   \n",
        "➅ 'Colab Notebooks'폴더 안에 꼭 'DFC615'라는 이름의 폴더를 만들어주세요  \n",
        "➆ 사본 ipynb 파일을 'DFC615' 폴더로 이동 시킨 후 열어주세요!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XASlXaWq9Bt1"
      },
      "source": [
        "## 수업 시작 전! 😀\n",
        "\n",
        "DFC615 깃헙 저장소에서 **실습 7강 ipynb**와 **test.json**를 다운로드 받으시고\n",
        "\n",
        "본인의 구글 드라이브 **Colab Notebooks/DFC615** 폴더로 옮겨주세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot7gqgprSGN-"
      },
      "source": [
        "### **구글 드라이브 연동** 😸"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUZnBnDPSJBu",
        "outputId": "2566d881-4a86-4f2a-e3b2-9678e235309e"
      },
      "source": [
        "# https://colab.research.google.com/drive/1EebE_t1s3hS6_oAvPH5DiOsLq3WlTTRZ?usp=sharing#scrollTo=OLrvpcBDTP97\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615') ## 현재 작업 환경으로 설정한 경로를 입력하세요"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhaRxS7eTz-"
      },
      "source": [
        "# 1. MRC on [Korqard](https://korquad.github.io/KorQuad%201.0/) with KoElectra\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGdjleI_imaa"
      },
      "source": [
        "## 1.1 Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0YNUOIt0YtF"
      },
      "source": [
        "#### **한국어 기계 독해 기반의 질의 응답 데이터셋**\n",
        "\n",
        "```\n",
        "KorQuAD 1.0: https://korquad.github.io/KorQuad%201.0/\n",
        "\n",
        "KorQuAD 2.0: https://korquad.github.io/\n",
        "```\n",
        "\n",
        "```\n",
        "KorQuAD는 한국어 기반의 기계 독해를 위해 제작한 데이터셋이며, 데이터셋은 지문과 질의 답변으로 이루어져 있습니다.\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtQyy-Pv7YpY"
      },
      "source": [
        "import json\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615')\n",
        "gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "#!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
        "#!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
        "# 이미 트레이닝과 va 데이터가 있으면 받지 말자"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7vcJjDHU7x1"
      },
      "source": [
        "output_dir = gdrive_path\n",
        "train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "dev_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_dev.json\")\n",
        "\n",
        "korquad_train = json.load(open(train_file,'r',encoding='utf-8'))    \n",
        "korquad_dev = json.load(open(dev_file,'r',encoding='utf-8'))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFcwlbPv8acf",
        "outputId": "75d5ad0a-ee47-4b5c-cd0d-dbe6a9fecaa9"
      },
      "source": [
        "print(korquad_train.keys())\n",
        "#print(korquad_train.values()) # 로딩 시간이 조금 걸려요\n",
        "\n",
        "## 0번째 타이틀에 대한 지문 집합\n",
        "print(korquad_train['version'])\n",
        "print(len(korquad_train['data']))\n",
        "print('-'*40)\n",
        "\n",
        "print(korquad_train['data'][0].keys()) # title, paragraphs (위키피디아를 기준으로 제작한 데이터임을 고려하기!)\n",
        "print(f\"title: {korquad_train['data'][0]['title']}\") # 제목은 하나 \n",
        "print('-'*40)\n",
        "print('paragraphs')\n",
        "print(korquad_train['data'][0]['paragraphs']) \n",
        "print(len(korquad_train['data'][0]['paragraphs'])) # 지문은 N개\n",
        "print('-'*40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['version', 'data'])\n",
            "KorQuAD_v1.0_train\n",
            "1420\n",
            "----------------------------------------\n",
            "dict_keys(['paragraphs', 'title'])\n",
            "title: 파우스트_서곡\n",
            "----------------------------------------\n",
            "paragraphs\n",
            "[{'qas': [{'answers': [{'text': '교향곡', 'answer_start': 54}], 'id': '6566495-0-0', 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}, {'answers': [{'text': '1악장', 'answer_start': 421}], 'id': '6566495-0-1', 'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'}, {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}], 'id': '6566495-0-2', 'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'}, {'answers': [{'text': '파우스트', 'answer_start': 15}], 'id': '6566518-0-0', 'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'}, {'answers': [{'text': '합창교향곡', 'answer_start': 354}], 'id': '6566518-0-1', 'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'}, {'answers': [{'text': '1839', 'answer_start': 0}], 'id': '5917067-0-0', 'question': '바그너가 파우스트를 처음으로 읽은 년도는?'}, {'answers': [{'text': '파리', 'answer_start': 410}], 'id': '5917067-0-1', 'question': '바그너가 처음 교향곡 작곡을 한 장소는?'}, {'answers': [{'text': '드레스덴', 'answer_start': 534}], 'id': '5917067-0-2', 'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}], 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'}, {'qas': [{'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '6566495-1-0', 'question': '바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '6566495-1-1', 'question': '잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?'}, {'answers': [{'text': '20루이의 금', 'answer_start': 345}], 'id': '6566495-1-2', 'question': '바그너는 다시 개정된 총보를 얼마를 받고 팔았는가?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '6566518-1-0', 'question': '파우스트 교향곡을 부활시킨 사람은?'}, {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '6566518-1-1', 'question': '파우스트 교향곡을 피아노 독주용으로 편곡한 사람은?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '5917067-1-0', 'question': '1악장을 부활시켜 연주한 사람은?'}, {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '5917067-1-1', 'question': '파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은?'}, {'answers': [{'text': '1840년', 'answer_start': 3}], 'id': '5917067-1-2', 'question': '리스트가 바그너와 알게 된 연도는?'}], 'context': '한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.'}, {'qas': [{'answers': [{'text': '주제, 동기', 'answer_start': 70}], 'id': '6566495-2-0', 'question': '서주에는 무엇이 암시되어 있는가?'}, {'answers': [{'text': '제1바이올린', 'answer_start': 148}], 'id': '6566495-2-1', 'question': '첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가?'}, {'answers': [{'text': '소나타 형식', 'answer_start': 272}], 'id': '6566495-2-2', 'question': '주요부는 어떤 형식으로 되어 있는가?'}, {'answers': [{'text': '저음 주제', 'answer_start': 102}], 'id': '6566518-2-0', 'question': '첫 부분의 주요주제를 암시하는 주제는?'}, {'answers': [{'text': 'D장조', 'answer_start': 409}], 'id': '6566518-2-1', 'question': '제2주제의 축소된 재현부의 조성은?'}, {'answers': [{'text': '4/4박자', 'answer_start': 35}], 'id': '5917067-2-0', 'question': '곡이 시작할때의 박자는?'}, {'answers': [{'text': '고뇌와 갈망 동기, 청춘의 사랑 동기', 'answer_start': 115}], 'id': '5917067-2-1', 'question': '이 곡의 주요 주제는?'}, {'answers': [{'text': 'D장조', 'answer_start': 409}], 'id': '5917067-2-2', 'question': '제 2주제에선 무슨 장조로 재현되는가?'}], 'context': '이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.'}]\n",
            "3\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D--843QA5OrI",
        "outputId": "c9e8ad2a-8547-4173-fbd5-1f9e9d046096"
      },
      "source": [
        "# 0번째 타이틀의 0번째 지문에 대한 {\"질의응답\": ~~ , \"지문\":   ~~}\n",
        "print(korquad_train['data'][0]['paragraphs'][0].keys()) # qas (question answering), context\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'])\n",
        "print(len(korquad_train['data'][0]['paragraphs'][0]['qas'])) # 질의 응답 쌍도 M개\n",
        "print('-'*40)\n",
        "print('context')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['context'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['qas', 'context'])\n",
            "[{'answers': [{'text': '교향곡', 'answer_start': 54}], 'id': '6566495-0-0', 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}, {'answers': [{'text': '1악장', 'answer_start': 421}], 'id': '6566495-0-1', 'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'}, {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}], 'id': '6566495-0-2', 'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'}, {'answers': [{'text': '파우스트', 'answer_start': 15}], 'id': '6566518-0-0', 'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'}, {'answers': [{'text': '합창교향곡', 'answer_start': 354}], 'id': '6566518-0-1', 'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'}, {'answers': [{'text': '1839', 'answer_start': 0}], 'id': '5917067-0-0', 'question': '바그너가 파우스트를 처음으로 읽은 년도는?'}, {'answers': [{'text': '파리', 'answer_start': 410}], 'id': '5917067-0-1', 'question': '바그너가 처음 교향곡 작곡을 한 장소는?'}, {'answers': [{'text': '드레스덴', 'answer_start': 534}], 'id': '5917067-0-2', 'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}]\n",
            "8\n",
            "----------------------------------------\n",
            "context\n",
            "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92DmNUgb5SqB",
        "outputId": "a311fbb5-1376-4f81-acb7-a2d23fe6d58f"
      },
      "source": [
        "print('-'*40)\n",
        "print('Question')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
        "print(\"Answer\")\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][0]['answers']) \n",
        "print('\\n')\n",
        "print('Question')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][1]['question'])\n",
        "print(\"Answer\")\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][1]['answers']) \n",
        "print('\\n')\n",
        "print('Question')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][2]['question'])\n",
        "print(\"Answer\")\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][2]['answers']) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Question\n",
            "바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
            "Answer\n",
            "[{'text': '교향곡', 'answer_start': 54}]\n",
            "\n",
            "\n",
            "Question\n",
            "바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?\n",
            "Answer\n",
            "[{'text': '1악장', 'answer_start': 421}]\n",
            "\n",
            "\n",
            "Question\n",
            "바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?\n",
            "Answer\n",
            "[{'text': '베토벤의 교향곡 9번', 'answer_start': 194}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVj5JvHZobsz"
      },
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "  <img src='http://drive.google.com/uc?export=view&id=1JdufTK6s3Mj4RhJ5xlBZNuBk0rif79AD' />\n",
        "</p> \n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFOxHl6Ziqtp",
        "outputId": "a1fb888d-5339-40f2-b18c-a63df284d8b9"
      },
      "source": [
        "train_cnt, val_cnt = 0, 0\n",
        "for idx, train in enumerate(korquad_train['data']):\n",
        "    title_name = train['title']\n",
        "    for paragraph in train['paragraphs']:\n",
        "        context=paragraph['context'] \n",
        "        for qas in paragraph['qas']:\n",
        "            train_cnt += 1\n",
        "\n",
        "for idx, train in enumerate(korquad_dev['data']): \n",
        "    title_name = train['title']\n",
        "    for paragraph in train['paragraphs']:\n",
        "        context=paragraph['context'] \n",
        "        for qas in paragraph['qas']:\n",
        "            val_cnt += 1\n",
        "print(f'Train: {train_cnt} | Val: {val_cnt}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 60407 | Val: 5774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW56ZsY80PmA"
      },
      "source": [
        "## 1.2 Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwOiJk31mos"
      },
      "source": [
        "\n",
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "(https://arxiv.org/pdf/1810.04805.pdf)\n",
        "\n",
        "<img src = \"https://dos-tacos.github.io/images/lynn/181229/8.PNG\" width = \"600\" >\n",
        "\n",
        "\n",
        "- 대용량의 텍스트 코퍼스를 통해서 사전 훈련된 모델로써 공개된 대표적인 언어 모델 중 하나.\n",
        "\n",
        "(https://github.com/google-research/bert)\n",
        "\n",
        "- BERT 사전 훈련의 특징\n",
        "\n",
        "1) MASKED LM\n",
        "- 기본 값 15% 무작위 확률로 [MASK] 입력 시퀀스에 대해서 wordpiece로 토큰화한 단어에 대해서 예측하도록 전체 문맥에 대해 양뱡향으로 정보를 습득하는 장점 극대화\n",
        "\n",
        "ex) \"나는 정보검색 [MASK]를 듣는다.\"\n",
        "\n",
        "ex) \"부산의 유명한 [MASK]에서 해수욕을 즐기고, 파도가 [MASK]지는 백사장을 거닐고 싶다.\"\n",
        "\n",
        "2) Next Sentence Prediction\n",
        "- 문장 단위의 입력 시퀀스를 넘어서, 문장과 문장 사이의 문맥을 파악할 수 있도록 하는 TASK를 포함\n",
        "\n",
        "- 50%는 실제로 다음에 오는 문장, 50%는 잘못된 문장으로 옳은 문장이 다음에 오도록 예측하게 함.\n",
        "\n",
        "- Special Tokens\n",
        "\n",
        "[CLS]: 입력 시퀀스의 처음으로 분류 정보를 담기 위한 임베딩 토큰. 분류 문제를 해결하는 경우 최종 hidden state 값을 담고 있음.\n",
        "\n",
        "[SEP]: 문장과 문장을 구분하여 하나의 시퀀스로 연결할 때 사용.\n",
        "\n",
        "ex) [CLS] 나는 배가 고프다. [SEP] 밥을 먹으러 가야겠다. [SEP] \n",
        "\n",
        "<img src = \"https://dos-tacos.github.io/images/lynn/181229/9.PNG\" width = \"600\" >\n",
        "\n",
        "```\n",
        "이미지 참조\n",
        "출처: https://dos-tacos.github.io/paper%20review/BERT/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpbCKNhk9F1m"
      },
      "source": [
        "#2. **사전 훈련된 KoELECTRA 활용하여 MRC 해결**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SulOW1Gp0r1m"
      },
      "source": [
        "**[ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB)**\n",
        "\n",
        "Google, Stanford에서 2020년에 발표한 ELECTRA 모델\n",
        "\n",
        "공개된 사전 훈련 모델을 Pytorch 기반으로 사용하기 쉽게 만든 HuggingFace, KoElectra 참조\n",
        "\n",
        "https://huggingface.co/transformers/\n",
        "https://awesomeopensource.com/project/monologg/KoELECTRA\n",
        "\n",
        "본 실습은 사전 훈련된 KoELECTRA를 바탕으로\n",
        "\n",
        "미세 조정 훈련을 통해 KorQuad 1.0의 질의 응답 문제를 해결\n",
        "\n",
        "\n",
        "<img src = https://user-images.githubusercontent.com/28896432/80024445-0f444e00-851a-11ea-9137-9da2abfd553d.png width = \"600\" >\n",
        "\n",
        "\n",
        "```\n",
        "@misc{park2020koelectra,\n",
        "  author = {Park, Jangwon},\n",
        "  title = {KoELECTRA: Pretrained ELECTRA Model for Korean},\n",
        "  year = {2020},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/monologg/KoELECTRA}}\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3pgUZDJ30xuL",
        "outputId": "fd3b93e2-5785-4a7c-c544-4e8a568a4cd2"
      },
      "source": [
        "# !pip install sentencepiece\n",
        "!pip install transformers==3.3.1\n",
        "!pip install seqeval\n",
        "!pip install fastprogress\n",
        "!pip install attrdict\n",
        "!pip install konlpy\n",
        "!pip uninstall pandas --yes\n",
        "!pip install pandas==1.1.5\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 15.9MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 18.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 16.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 358kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 368kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 378kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 389kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 409kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 419kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 430kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 440kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 450kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 460kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 471kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 481kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 491kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 501kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 512kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 522kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 532kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 542kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 552kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 563kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 573kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 583kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 593kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 604kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 614kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 624kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 634kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 645kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 655kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 665kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 675kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 686kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 696kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 706kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 716kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 727kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 737kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 747kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 757kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 768kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 778kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 788kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 798kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 808kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 819kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 829kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 839kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 849kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 860kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 870kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 880kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 890kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 901kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 911kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 921kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 931kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 942kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 952kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 962kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 972kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 983kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 993kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 28.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=4a15f93cdc7e5f5522b07b5e7c2e09306cb215d889acf5abe1803c93443a6751\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress) (1.19.5)\n",
            "Collecting attrdict\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Installing collected packages: JPype1, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Uninstalling pandas-1.1.5:\n",
            "  Successfully uninstalled pandas-1.1.5\n",
            "Collecting pandas==1.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/70/e8eee0cbddf926bf51958c7d6a86bc69167c300fa2ba8e592330a2377d1b/pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "Successfully installed pandas-1.1.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECEO0b3Z_b3h"
      },
      "source": [
        "## 2.1 필요한 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBcdP03_bYq"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from collections import Counter\n",
        "import sys\n",
        "import argparse\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from attrdict import AttrDict\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    ElectraForQuestionAnswering,\n",
        "    XLMRobertaForQuestionAnswering,\n",
        "    \n",
        "    ElectraTokenizer,\n",
        "    XLMRobertaTokenizer,\n",
        "\n",
        "    ElectraConfig,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "from transformers import pipeline\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WPNgimuaKt"
      },
      "source": [
        "## 2.2 Fine-Tuning을 위한 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnYl2fFuttPZ"
      },
      "source": [
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "MODEL_FOR_QUESTION_ANSWERING = {\n",
        "    \"koelectra-base-v3\": ElectraForQuestionAnswering,\n",
        "    \"koelectra-small-v3\": ElectraForQuestionAnswering,\n",
        "}\n",
        "TOKENIZER_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraTokenizer,\n",
        "    \"koelectra-small-v3\": ElectraTokenizer,\n",
        "}\n",
        "CONFIG_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraConfig,\n",
        "    \"koelectra-small-v3\": ElectraConfig,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug0At9pUuIlb"
      },
      "source": [
        "\n",
        "'''KorQuAD v1.0에 대한 공식 평가 스크립트 '''\n",
        "'''본 스크립트는 SQuAD v1.1 평가 스크립트 https://rajpurkar.github.io/SQuAD-explorer/ 를 바탕으로 작성됨.'''\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    def remove_(text):\n",
        "        ''' 불필요한 기호 제거 '''\n",
        "        text = re.sub(\"'\", \" \", text)\n",
        "        text = re.sub('\"', \" \", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \" \", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \" \", text)\n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \" \", text)\n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \" \", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \" \", text)\n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(remove_(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    # F1 by character\n",
        "    prediction_Char = []\n",
        "    for tok in prediction_tokens:\n",
        "        now = [a for a in tok]\n",
        "        prediction_Char.extend(now)\n",
        "\n",
        "    ground_truth_Char = []\n",
        "    for tok in ground_truth_tokens:\n",
        "        now = [a for a in tok]\n",
        "        ground_truth_Char.extend(now)\n",
        "\n",
        "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_Char)\n",
        "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "    return {'official_exact_match': exact_match, 'official_f1': f1}\n",
        "\n",
        "\n",
        "def eval_during_train(args, step):\n",
        "    expected_version = 'KorQuAD_v1.0'\n",
        "\n",
        "    dataset_file = os.path.join(args.data_dir, args.predict_file)\n",
        "    prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(step))\n",
        "\n",
        "    with open(dataset_file) as dataset_f:\n",
        "        dataset_json = json.load(dataset_f)\n",
        "        read_version = \"_\".join(dataset_json['version'].split(\"_\")[:-1])\n",
        "        if (read_version != expected_version):\n",
        "            print('Evaluation expects ' + expected_version +\n",
        "                  ', but got dataset with ' + read_version,\n",
        "                  file=sys.stderr)\n",
        "        dataset = dataset_json['data']\n",
        "    with open(prediction_file) as prediction_f:\n",
        "        predictions = json.load(prediction_f)\n",
        "\n",
        "    return evaluate(dataset, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gruobhuH9FME"
      },
      "source": [
        "## 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98UVT9r617XF"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Train batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 1\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    # Added here for reproductibility\n",
        "    set_seed(args)\n",
        "\n",
        "    for epoch in mb:\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n",
        "                if args.version_2_with_negative:\n",
        "                    inputs.update({\"is_impossible\": batch[7]})\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    if args.evaluate_during_training:\n",
        "                        results = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "                        for key in sorted(results.keys()):\n",
        "                            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # Save model checkpoint\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch+1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluation(args, model, tokenizer, global_step=None):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(global_step))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "            start_logits = outputs[0] # bs 512\n",
        "            end_logits = outputs[1] # torch.Tensor\n",
        "            \n",
        "            \n",
        "            # print(len(start_logits)\n",
        "            # print(len(start_logits[0]))\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "        # print(len(end_logits))\n",
        "        # print(len(end_logits[0]))\n",
        "        \n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            start_logit = start_logits[i].cpu().tolist()\n",
        "            end_logit = end_logits[i].cpu().tolist()\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            eval_feature = features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            result = SquadResult(unique_id, start_logit, end_logit)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(global_step))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(global_step))\n",
        "\n",
        "    if args.version_2_with_negative:\n",
        "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(global_step))\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        args.do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        args.verbose_logging,\n",
        "        args.version_2_with_negative,\n",
        "        args.null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    # Write the result\n",
        "    # Write the evaluation result on file\n",
        "    output_dir = os.path.join(args.output_dir, 'eval')\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_eval_file = os.path.join(output_dir, \"eval_result_{}_{}.txt\".format(list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                                                                               global_step))\n",
        "\n",
        "    with open(output_eval_file, \"w\", encoding='utf-8') as f:\n",
        "        official_eval_results = eval_during_train(args, step=global_step)\n",
        "        results.update(official_eval_results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "\n",
        "        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n",
        "            try:\n",
        "                import tensorflow_datasets as tfds\n",
        "            except ImportError:\n",
        "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
        "\n",
        "            if args.version_2_with_negative:\n",
        "                logger.warn(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
        "\n",
        "            tfds_examples = tfds.load(\"squad\")\n",
        "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
        "        else:\n",
        "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
        "            if evaluate:\n",
        "                examples = processor.get_dev_examples(os.path.join(args.data_dir),\n",
        "                                                      filename=args.predict_file)\n",
        "            else:\n",
        "                examples = processor.get_train_examples(os.path.join(args.data_dir),\n",
        "                                                        filename=args.train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=args.threads,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Read from config file and make args\n",
        "    logger.info(\"Training/evaluation parameters {}\".format(args))\n",
        "\n",
        "    args.output_dir = os.path.join(args.output_dir)\n",
        "\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "\n",
        "    logging.getLogger(\"transformers.data.metrics.squad_metrics\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = CONFIG_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "    )\n",
        "    tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "    )\n",
        "    model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "    # GPU or CPU\n",
        "    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        checkpoints = list(\n",
        "            os.path.dirname(c)\n",
        "            for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "        if not args.eval_all_checkpoints:\n",
        "            checkpoints = checkpoints[-1:]\n",
        "        else:\n",
        "            logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1]\n",
        "            model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as f_w:\n",
        "            for key in sorted(results.keys()):\n",
        "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
        "                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGXlI9EF8b7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6dc1030b3cab4fd0a0a1cf527675c3f1",
            "9037fd5a4840411d8fc1fe8ce05f5818",
            "accdbc37b53845beb77223c5ede78f47",
            "11afcdadad37455185390d1785e24961",
            "3637e68b16804f34a951c1bab36c395c",
            "884054df11f54a4582bdb618ee7d17ae",
            "fb9c69c2d8a24aa0ac24ed634f6f4232",
            "6369d806d4c249b3bcc00de3ee0b4cfc",
            "ef4d610738824273b044f85dc667d6d8",
            "904bc6ce8b8041338c7baae3e103c62b",
            "499cd19011414d4bb7c5b66a0468dd40",
            "695abbba60044a2db36ae29f4c25c4dc",
            "4d7c41177be44418811094367b6bc65c",
            "dc050a2c905f4a87a9d734c4adb84fd6",
            "d0cb7bc6ea464b0ca077c53200061481",
            "c7205ad5fdb642a1bbf2367879017d7d",
            "f5e9c9bb9b0d400dad005c7dc0eab50e",
            "ee562a8dd4d14ba79f153675ff21e1de",
            "332b046a6e724a4596a44e55567fa681",
            "a3b013b9ac8046e9ac62919ad21989f6",
            "2cac23edcb2047e68a522ce0944596e7",
            "dd7750d3b14c4466a02246bb9b6191b9",
            "d1bd70862f464526b6bb343a7c9e5dfb",
            "50a054c4300d407ca06769890fe8189c",
            "6dddce1d73034c83b05d4cedfd3b6b3d",
            "034780d6bd934665adc4978445683616",
            "8ed9f01817ec477d96e6f539cf7f00bc",
            "f1edea9c6210461780e5437b7774ede1",
            "93c8f830a50b4c5590c4efbfc1375a13",
            "f2ac93b9f0784f06ba3ebe1a9227b832",
            "4a534c702bc546cf9684397de106ed10",
            "eeb7fc437e614aabb803396b6eec064d"
          ]
        },
        "id": "IOaFLMZuAVPX",
        "outputId": "dc6cf03f-af5a-480f-e959-cd8cea99608a"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "## 명령행 인터페이스, 사전 훈련된 모델을 로드하여 훈련하고 평가하는데까지 필요한 인자를 사용자 정의 인자로 설정하는 내용을 담고 있음.\n",
        "## 사용법: python example.py --dataset_name squad.json .... \n",
        "## 주피터 환경을 위한 인자 설정법 \n",
        "# \"output_dir\": \"/content/gdrive/My Drive/Colab Notebooks/DFC615/koelectra-small-korquad-ckpt\",\n",
        "\n",
        "    os.chdir('/content/gdrive/MyDrive/Colab Notebooks/DFC615')\n",
        "    gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "    data_dir = gdrive_path\n",
        "    output_dir = os.path.join(gdrive_path, \"output\")\n",
        "    # train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "    # dev_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_dev.json\")\n",
        "\n",
        "    hyper_para={\n",
        "    \"task\": \"korquad\",\n",
        "    \"data_dir\": data_dir,\n",
        "    \"ckpt_dir\": \"ckpt\",\n",
        "    \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
        "    \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
        "    \"threads\": 4,\n",
        "    \"version_2_with_negative\": False,\n",
        "    \"null_score_diff_threshold\": 0.0,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"doc_stride\": 128,\n",
        "    \"max_query_length\": 64,\n",
        "    \"max_answer_length\": 30,\n",
        "    \"n_best_size\": 20,\n",
        "    \"verbose_logging\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"eval_all_checkpoints\": True,\n",
        "    \"save_optimizer\": False,\n",
        "    \"do_lower_case\": False,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"num_train_epochs\": 12,               ## epoch를 10 에서 12로 수정\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"warmup_proportion\": 0,\n",
        "    \"max_steps\": -1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"no_cuda\": False,\n",
        "    \"model_type\": \"koelectra-small-v3\",\n",
        "    \"model_name_or_path\": \"monologg/koelectra-small-v3-discriminator\",\n",
        "    \"output_dir\": output_dir,\n",
        "    \"seed\": 42,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"eval_batch_size\": 64,\n",
        "    \"logging_steps\": 3000,      \n",
        "    \"save_steps\": 3000,\n",
        "\n",
        "\n",
        "\n",
        "    \"learning_rate\": 2e-5,    #small : 5e-5 base : 2e-5\n",
        "    \"num_attention_heads\" : 12,\n",
        "    \"hidden_size\" : 512,     # small 256, base 768\n",
        "\n",
        "    }\n",
        "\n",
        "    main(AttrDict(hyper_para))\n",
        "\n",
        "    # hyper_para['learning_rate']\n",
        "    # hyper_para.learning_rate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140218793754256 acquired on /root/.cache/torch/transformers/a95c1ddd5ce62adf8a4e5a75f70cb973b79c2d82705edc611de7b868ab917973.158ddd8c5effdf89c9e973250fe1db004e648ff1295bb63fd3abb0709141c558.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dc1030b3cab4fd0a0a1cf527675c3f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=458.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140218793754256 released on /root/.cache/torch/transformers/a95c1ddd5ce62adf8a4e5a75f70cb973b79c2d82705edc611de7b868ab917973.158ddd8c5effdf89c9e973250fe1db004e648ff1295bb63fd3abb0709141c558.lock\n",
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140221734267344 acquired on /root/.cache/torch/transformers/40bac8751309ca787dded2e47050b0ebb2b1292b04f5d7f6d8ff649c0164a99a.e060f8ca1da74aef5ab085f278fb3612042e7e4fc97a4917ed87540d3966ead6.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef4d610738824273b044f85dc667d6d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263326.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140221734267344 released on /root/.cache/torch/transformers/40bac8751309ca787dded2e47050b0ebb2b1292b04f5d7f6d8ff649c0164a99a.e060f8ca1da74aef5ab085f278fb3612042e7e4fc97a4917ed87540d3966ead6.lock\n",
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140218793192720 acquired on /root/.cache/torch/transformers/8e1af0a0f961eda7ae6a6a31d7b3a2b508ffba755d7ca857a0acd95ce4db0d8d.548bc4daa128698b4e44ee737f52215fad60b4eaa50f02442759fdc39cfbdb9a.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5e9c9bb9b0d400dad005c7dc0eab50e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=61.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140218793192720 released on /root/.cache/torch/transformers/8e1af0a0f961eda7ae6a6a31d7b3a2b508ffba755d7ca857a0acd95ce4db0d8d.548bc4daa128698b4e44ee737f52215fad60b4eaa50f02442759fdc39cfbdb9a.lock\n",
            "06/21/2021 01:17:22 - INFO - filelock -   Lock 140218793315984 acquired on /root/.cache/torch/transformers/6e92e54c54ba31d8dbbb109128b5fc6b9904441c61e0eecf2b746dc118b92ce6.e907291bb60f0f7517ed4002a253cf54e79e3dd8181ba822b901b1c6f57a36ae.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dddce1d73034c83b05d4cedfd3b6b3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=56577499.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:17:24 - INFO - filelock -   Lock 140218793315984 released on /root/.cache/torch/transformers/6e92e54c54ba31d8dbbb109128b5fc6b9904441c61e0eecf2b746dc118b92ce6.e907291bb60f0f7517ed4002a253cf54e79e3dd8181ba822b901b1c6f57a36ae.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "06/21/2021 01:17:36 - INFO - __main__ -   Training/evaluation parameters AttrDict({'task': 'korquad', 'data_dir': '/content/gdrive/My Drive/Colab Notebooks/DFC615', 'ckpt_dir': 'ckpt', 'train_file': 'KorQuAD_v1.0_train.json', 'predict_file': 'KorQuAD_v1.0_dev.json', 'threads': 4, 'version_2_with_negative': False, 'null_score_diff_threshold': 0.0, 'max_seq_length': 512, 'doc_stride': 128, 'max_query_length': 64, 'max_answer_length': 30, 'n_best_size': 20, 'verbose_logging': True, 'overwrite_output_dir': True, 'evaluate_during_training': True, 'eval_all_checkpoints': True, 'save_optimizer': False, 'do_lower_case': False, 'do_train': True, 'do_eval': True, 'num_train_epochs': 12, 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'max_steps': -1, 'max_grad_norm': 1.0, 'no_cuda': False, 'model_type': 'koelectra-small-v3', 'model_name_or_path': 'monologg/koelectra-small-v3-discriminator', 'output_dir': '/content/gdrive/My Drive/Colab Notebooks/DFC615/output', 'seed': 42, 'train_batch_size': 32, 'eval_batch_size': 64, 'logging_steps': 3000, 'save_steps': 3000, 'learning_rate': 2e-05, 'num_attention_heads': 12, 'hidden_size': 512, 'device': 'cuda'})\n",
            "06/21/2021 01:17:36 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_train_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 01:18:48 - INFO - __main__ -   ***** Running training *****\n",
            "06/21/2021 01:18:48 - INFO - __main__ -     Num examples = 64386\n",
            "06/21/2021 01:18:48 - INFO - __main__ -     Num Epochs = 12\n",
            "06/21/2021 01:18:48 - INFO - __main__ -     Train batch size per GPU = 32\n",
            "06/21/2021 01:18:48 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "06/21/2021 01:18:48 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "06/21/2021 01:18:48 - INFO - __main__ -     Total optimization steps = 24156\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Epoch 1 done<p>Epoch 2 done<p>Epoch 3 done<p>Epoch 4 done<p>Epoch 5 done<p>Epoch 6 done<p>Epoch 7 done<p>Epoch 8 done<p>Epoch 9 done<p>Epoch 10 done<p>Epoch 11 done<p>Epoch 12 done"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:51:08 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 01:51:15 - INFO - __main__ -   ***** Running evaluation 3000 *****\n",
            "06/21/2021 01:51:15 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 01:51:15 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 01:52:01 - INFO - __main__ -     Evaluation done in total 46.545487 secs (0.007096 sec per example)\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     HasAns_exact = 77.0696224454451\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     HasAns_f1 = 83.47743508463992\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     best_exact = 77.0696224454451\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     best_f1 = 83.47743508463992\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     exact = 77.0696224454451\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     f1 = 83.47743508463992\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     official_exact_match = 77.2081745756841\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     official_f1 = 87.56569747255021\n",
            "06/21/2021 01:52:16 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 01:52:19 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-3000\n",
            "06/21/2021 02:24:44 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 02:24:49 - INFO - __main__ -   ***** Running evaluation 6000 *****\n",
            "06/21/2021 02:24:49 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 02:24:49 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 02:25:35 - INFO - __main__ -     Evaluation done in total 46.471103 secs (0.007085 sec per example)\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     HasAns_exact = 80.16972635954278\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     HasAns_f1 = 85.86776470652472\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     best_exact = 80.16972635954278\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     best_f1 = 85.86776470652472\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     exact = 80.16972635954278\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     f1 = 85.86776470652472\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     official_exact_match = 80.30827848978178\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     official_f1 = 89.76091472554074\n",
            "06/21/2021 02:25:50 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 02:25:54 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-6000\n",
            "06/21/2021 02:58:20 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 02:58:25 - INFO - __main__ -   ***** Running evaluation 9000 *****\n",
            "06/21/2021 02:58:25 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 02:58:25 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 02:59:11 - INFO - __main__ -     Evaluation done in total 46.453074 secs (0.007082 sec per example)\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     HasAns_exact = 81.17422930377555\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     HasAns_f1 = 86.83706567276943\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     best_exact = 81.17422930377555\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     best_f1 = 86.83706567276943\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     exact = 81.17422930377555\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     f1 = 86.83706567276943\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     official_exact_match = 81.31278143401455\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     official_f1 = 90.66064717141485\n",
            "06/21/2021 02:59:26 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 02:59:31 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-9000\n",
            "06/21/2021 03:31:55 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 03:31:59 - INFO - __main__ -   ***** Running evaluation 12000 *****\n",
            "06/21/2021 03:31:59 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 03:31:59 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 03:32:46 - INFO - __main__ -     Evaluation done in total 46.840892 secs (0.007141 sec per example)\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     HasAns_exact = 81.39937651541392\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     HasAns_f1 = 87.08212427990757\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     best_exact = 81.39937651541392\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     best_f1 = 87.08212427990757\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     exact = 81.39937651541392\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     f1 = 87.08212427990757\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     official_exact_match = 81.53792864565293\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     official_f1 = 90.82832566616604\n",
            "06/21/2021 03:33:01 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 03:33:03 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-12000\n",
            "06/21/2021 04:05:25 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 04:05:30 - INFO - __main__ -   ***** Running evaluation 15000 *****\n",
            "06/21/2021 04:05:30 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 04:05:30 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 04:06:16 - INFO - __main__ -     Evaluation done in total 46.396587 secs (0.007074 sec per example)\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     HasAns_exact = 81.46865258053343\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     HasAns_f1 = 87.0691142556402\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     best_exact = 81.46865258053343\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     best_f1 = 87.0691142556402\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     exact = 81.46865258053343\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     f1 = 87.0691142556402\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     official_exact_match = 81.6245237270523\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     official_f1 = 90.81340330829072\n",
            "06/21/2021 04:06:31 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 04:06:34 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-15000\n",
            "06/21/2021 04:38:55 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 04:38:59 - INFO - __main__ -   ***** Running evaluation 18000 *****\n",
            "06/21/2021 04:38:59 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 04:38:59 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 04:39:46 - INFO - __main__ -     Evaluation done in total 46.220066 secs (0.007047 sec per example)\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     HasAns_exact = 81.45133356425355\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     HasAns_f1 = 87.17431126980034\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     best_exact = 81.45133356425355\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     best_f1 = 87.17431126980034\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     exact = 81.45133356425355\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     f1 = 87.17431126980034\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     official_exact_match = 81.60720471077242\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     official_f1 = 90.87128249490453\n",
            "06/21/2021 04:40:00 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 04:40:03 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-18000\n",
            "06/21/2021 05:12:20 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:12:25 - INFO - __main__ -   ***** Running evaluation 21000 *****\n",
            "06/21/2021 05:12:25 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:12:25 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:13:11 - INFO - __main__ -     Evaluation done in total 46.206530 secs (0.007045 sec per example)\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     HasAns_exact = 81.74575684101143\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     HasAns_f1 = 87.28245765522412\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     best_exact = 81.74575684101143\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     best_f1 = 87.28245765522412\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     exact = 81.74575684101143\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     f1 = 87.28245765522412\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     official_exact_match = 81.9016279875303\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     official_f1 = 91.01417538234288\n",
            "06/21/2021 05:13:26 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 05:13:28 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-21000\n",
            "06/21/2021 05:45:49 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:45:54 - INFO - __main__ -   ***** Running evaluation 24000 *****\n",
            "06/21/2021 05:45:54 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:45:54 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:46:40 - INFO - __main__ -     Evaluation done in total 46.707450 secs (0.007121 sec per example)\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     HasAns_exact = 81.86698995497056\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     HasAns_f1 = 87.3806283526328\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     HasAns_total = 5774\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     best_exact = 81.86698995497056\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     best_exact_thresh = 0.0\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     best_f1 = 87.3806283526328\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     best_f1_thresh = 0.0\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     exact = 81.86698995497056\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     f1 = 87.3806283526328\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     official_exact_match = 82.02286110148944\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     official_f1 = 91.04947997451221\n",
            "06/21/2021 05:46:55 - INFO - __main__ -     total = 5774\n",
            "06/21/2021 05:46:57 - INFO - __main__ -   Saving model checkpoint to /content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-24000\n",
            "06/21/2021 05:48:39 - INFO - __main__ -    global_step = 24157, average loss = 0.5851052645402081\n",
            "06/21/2021 05:48:39 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-12000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-15000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-18000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-21000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-24000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-3000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-6000', '/content/gdrive/My Drive/Colab Notebooks/DFC615/output/checkpoint-9000']\n",
            "06/21/2021 05:48:39 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:48:44 - INFO - __main__ -   ***** Running evaluation 12000 *****\n",
            "06/21/2021 05:48:44 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:48:44 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:49:30 - INFO - __main__ -     Evaluation done in total 46.648663 secs (0.007112 sec per example)\n",
            "06/21/2021 05:49:45 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:49:50 - INFO - __main__ -   ***** Running evaluation 15000 *****\n",
            "06/21/2021 05:49:50 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:49:50 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:50:37 - INFO - __main__ -     Evaluation done in total 46.801894 secs (0.007136 sec per example)\n",
            "06/21/2021 05:50:51 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:50:55 - INFO - __main__ -   ***** Running evaluation 18000 *****\n",
            "06/21/2021 05:50:55 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:50:56 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:51:42 - INFO - __main__ -     Evaluation done in total 46.883065 secs (0.007148 sec per example)\n",
            "06/21/2021 05:51:56 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:52:01 - INFO - __main__ -   ***** Running evaluation 21000 *****\n",
            "06/21/2021 05:52:01 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:52:01 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:47<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:52:49 - INFO - __main__ -     Evaluation done in total 47.356810 secs (0.007220 sec per example)\n",
            "06/21/2021 05:53:02 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:53:07 - INFO - __main__ -   ***** Running evaluation 24000 *****\n",
            "06/21/2021 05:53:07 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:53:07 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:53:54 - INFO - __main__ -     Evaluation done in total 46.780736 secs (0.007132 sec per example)\n",
            "06/21/2021 05:54:08 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:54:13 - INFO - __main__ -   ***** Running evaluation 3000 *****\n",
            "06/21/2021 05:54:13 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:54:13 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:55:00 - INFO - __main__ -     Evaluation done in total 46.902613 secs (0.007151 sec per example)\n",
            "06/21/2021 05:55:14 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:55:19 - INFO - __main__ -   ***** Running evaluation 6000 *****\n",
            "06/21/2021 05:55:19 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:55:19 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:46<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:56:06 - INFO - __main__ -     Evaluation done in total 46.912911 secs (0.007152 sec per example)\n",
            "06/21/2021 05:56:20 - INFO - __main__ -   Loading features from cached file /content/gdrive/My Drive/Colab Notebooks/DFC615/cached_dev_koelectra-small-v3-discriminator_512\n",
            "06/21/2021 05:56:25 - INFO - __main__ -   ***** Running evaluation 9000 *****\n",
            "06/21/2021 05:56:25 - INFO - __main__ -     Num examples = 6559\n",
            "06/21/2021 05:56:25 - INFO - __main__ -     Batch size = 64\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [103/103 00:47<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "06/21/2021 05:57:12 - INFO - __main__ -     Evaluation done in total 47.215489 secs (0.007199 sec per example)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpQFg7WW249b"
      },
      "source": [
        "- https://github.com/monologg/KoBERT-KorQuAD\n",
        "- https://github.com/Beomi/KcBERT\n",
        "- https://github.com/Beomi/KcBERT-finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knldPNL8uszZ"
      },
      "source": [
        "## 2.4 Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ONC6b2fD2go",
        "outputId": "952208a8-c984-4b88-e277-f00dff5db6de"
      },
      "source": [
        "test_file= os.path.join(gdrive_path,'test.json')\n",
        "test_json = json.load(open(test_file,'r', encoding='utf-8'))\n",
        "\n",
        "cont_list = []\n",
        "ques_list = []\n",
        "\n",
        "for sample in test_json['data']:\n",
        "   \n",
        "    context = sample['context']\n",
        "    len_context = len(context)\n",
        "    question = sample['question']\n",
        "\n",
        "    cont_list.append(context)\n",
        "    ques_list.append(question)\n",
        "\n",
        "    print(f'length context {len_context}')\n",
        "    print(f'context {context}')\n",
        "    print(f'question {question}')    \n",
        "    print('\\n')\n",
        "    \n",
        "print(len(test_json['data']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OlJpiSA0L17",
        "outputId": "bf807644-85c3-4601-f156-729567f7a317"
      },
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained(\"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-15000\")\n",
        "model = ElectraForQuestionAnswering.from_pretrained(\"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-15000\")\n",
        "prediction_qa = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
        "\n",
        "context_question = {\n",
        "    'question':'도지코인은 누구의 영향을 받나요?',\n",
        "    'context':'일론머스크의 한마디에 도지코인은 오르락 내리락 오늘도 그의 트위터를 기다린다. 빨간맛 가즈아'\n",
        "    \n",
        "}\n",
        "answer = prediction_qa(context_question)\n",
        "print(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'score': 0.4991209805011749, 'start': 0, 'end': 6, 'answer': '일론머스크의'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDpiiqpov--1"
      },
      "source": [
        "## 2.5 기말고사 제출 리더보드용 파일생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1FrO0IEyGb8"
      },
      "source": [
        "def normalize_answer_mod(s):                    ##답변에서 조사를 제거하기 위해서 answer normalize에서 일부 수정한다\n",
        "    def remove_(text):\n",
        "        ''' 불필요한 기호 제거 '''\n",
        "        text = re.sub(\"'\", \"\", text)\n",
        "        text = re.sub('\"', \"\", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \"\", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \"\", text)\n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \"\", text)\n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \"\", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \"\", text)\n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(remove_(s))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I918lQeYL9gM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb79c35-6cb9-49df-daa7-872359ffb589"
      },
      "source": [
        "my_answer = {}\n",
        "my_answer['id']=[]\n",
        "my_answer['prediction_text']=[]\n",
        "\n",
        "for idx, sample in enumerate(test_json['data']):\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    answer_dict = prediction_qa({'question':question,'context': context})\n",
        "    predict_text = normalize_answer_mod(answer_dict[\"answer\"])\n",
        "    \n",
        "    #print(f'context {context}')\n",
        "    #print(f'question {question}')\n",
        "    print(f'idx {idx}')\n",
        "    print(f'predict_text {predict_text}')\n",
        "    print('\\n')\n",
        "    \n",
        "    my_answer['id'].append(str(idx + 1))\n",
        "    my_answer['prediction_text'].append(predict_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idx 0\n",
            "predict_text 치안관리처벌법에\n",
            "\n",
            "\n",
            "idx 1\n",
            "predict_text 2013년\n",
            "\n",
            "\n",
            "idx 2\n",
            "predict_text 빛은\n",
            "\n",
            "\n",
            "idx 3\n",
            "predict_text 타이베리움을\n",
            "\n",
            "\n",
            "idx 4\n",
            "predict_text 11월 3일\n",
            "\n",
            "\n",
            "idx 5\n",
            "predict_text 대구 경북지역에서\n",
            "\n",
            "\n",
            "idx 6\n",
            "predict_text 10로\n",
            "\n",
            "\n",
            "idx 7\n",
            "predict_text save me\n",
            "\n",
            "\n",
            "idx 8\n",
            "predict_text 제3차 하리코프 공방전에\n",
            "\n",
            "\n",
            "idx 9\n",
            "predict_text 동대문아파트는\n",
            "\n",
            "\n",
            "idx 10\n",
            "predict_text 사라예보에\n",
            "\n",
            "\n",
            "idx 11\n",
            "predict_text 경영자\n",
            "\n",
            "\n",
            "idx 12\n",
            "predict_text 자신의 개인 활동에\n",
            "\n",
            "\n",
            "idx 13\n",
            "predict_text 다케조에 신이치로 竹添進一郎에겐\n",
            "\n",
            "\n",
            "idx 14\n",
            "predict_text 빛의 제국이\n",
            "\n",
            "\n",
            "idx 15\n",
            "predict_text 섭외민사관계 법률적용법 섭외민사관계법률적용법은\n",
            "\n",
            "\n",
            "idx 16\n",
            "predict_text 김대중 김종필\n",
            "\n",
            "\n",
            "idx 17\n",
            "predict_text 그랑프리의\n",
            "\n",
            "\n",
            "idx 18\n",
            "predict_text 이완과\n",
            "\n",
            "\n",
            "idx 19\n",
            "predict_text 22조 9000억원으로\n",
            "\n",
            "\n",
            "idx 20\n",
            "predict_text 600000장이\n",
            "\n",
            "\n",
            "idx 21\n",
            "predict_text 음악적 형태와 장르 재즈는 음악적 예술 형태이다\n",
            "\n",
            "\n",
            "idx 22\n",
            "predict_text 시카고 블루스라\n",
            "\n",
            "\n",
            "idx 23\n",
            "predict_text 세계일보에서는\n",
            "\n",
            "\n",
            "idx 24\n",
            "predict_text 블록체인 blockchain\n",
            "\n",
            "\n",
            "idx 25\n",
            "predict_text 지난해 4월에\n",
            "\n",
            "\n",
            "idx 26\n",
            "predict_text 부산에서\n",
            "\n",
            "\n",
            "idx 27\n",
            "predict_text 30일간\n",
            "\n",
            "\n",
            "idx 28\n",
            "predict_text 마스크를\n",
            "\n",
            "\n",
            "idx 29\n",
            "predict_text 샤이크 미스킨에서는\n",
            "\n",
            "\n",
            "idx 30\n",
            "predict_text 15년까지만\n",
            "\n",
            "\n",
            "idx 31\n",
            "predict_text sbs에\n",
            "\n",
            "\n",
            "idx 32\n",
            "predict_text 1992년\n",
            "\n",
            "\n",
            "idx 33\n",
            "predict_text 장진\n",
            "\n",
            "\n",
            "idx 34\n",
            "predict_text 요르단 강\n",
            "\n",
            "\n",
            "idx 35\n",
            "predict_text 서울올림픽주경기장에서\n",
            "\n",
            "\n",
            "idx 36\n",
            "predict_text 이진호 李軫鎬\n",
            "\n",
            "\n",
            "idx 37\n",
            "predict_text 업의 성질의\n",
            "\n",
            "\n",
            "idx 38\n",
            "predict_text 정수기 cf가\n",
            "\n",
            "\n",
            "idx 39\n",
            "predict_text 차범근 축구교실을\n",
            "\n",
            "\n",
            "idx 40\n",
            "predict_text 송파구 장미아파트로\n",
            "\n",
            "\n",
            "idx 41\n",
            "predict_text 4월 3일\n",
            "\n",
            "\n",
            "idx 42\n",
            "predict_text 300만 원\n",
            "\n",
            "\n",
            "idx 43\n",
            "predict_text 이회창은\n",
            "\n",
            "\n",
            "idx 44\n",
            "predict_text 진상은 이렇다라는\n",
            "\n",
            "\n",
            "idx 45\n",
            "predict_text 평민당사를\n",
            "\n",
            "\n",
            "idx 46\n",
            "predict_text 이븐 알나딤 ibn\n",
            "\n",
            "\n",
            "idx 47\n",
            "predict_text 문헌 文憲이라는\n",
            "\n",
            "\n",
            "idx 48\n",
            "predict_text 2007년 12월 7일\n",
            "\n",
            "\n",
            "idx 49\n",
            "predict_text 윤상 콘서트play\n",
            "\n",
            "\n",
            "idx 50\n",
            "predict_text 하프연주에\n",
            "\n",
            "\n",
            "idx 51\n",
            "predict_text 대포동 2호를\n",
            "\n",
            "\n",
            "idx 52\n",
            "predict_text 인천을\n",
            "\n",
            "\n",
            "idx 53\n",
            "predict_text 한겨레에서는\n",
            "\n",
            "\n",
            "idx 54\n",
            "predict_text 2012년 7월 5일\n",
            "\n",
            "\n",
            "idx 55\n",
            "predict_text 화합적취 和合積聚 즉 화합하여 쌓인다는 뜻이다\n",
            "\n",
            "\n",
            "idx 56\n",
            "predict_text 김홍집을\n",
            "\n",
            "\n",
            "idx 57\n",
            "predict_text 환경권이\n",
            "\n",
            "\n",
            "idx 58\n",
            "predict_text 우리 집에 왜 왔니에\n",
            "\n",
            "\n",
            "idx 59\n",
            "predict_text seethrough hmd를\n",
            "\n",
            "\n",
            "idx 60\n",
            "predict_text 도둑들에서\n",
            "\n",
            "\n",
            "idx 61\n",
            "predict_text 알렉산드르 1세는\n",
            "\n",
            "\n",
            "idx 62\n",
            "predict_text 최동훈\n",
            "\n",
            "\n",
            "idx 63\n",
            "predict_text 1천100억 달러를\n",
            "\n",
            "\n",
            "idx 64\n",
            "predict_text 승해 勝解\n",
            "\n",
            "\n",
            "idx 65\n",
            "predict_text 내부 기생충이\n",
            "\n",
            "\n",
            "idx 66\n",
            "predict_text 점프 블루스\n",
            "\n",
            "\n",
            "idx 67\n",
            "predict_text 스크린 scrin을\n",
            "\n",
            "\n",
            "idx 68\n",
            "predict_text 김구는\n",
            "\n",
            "\n",
            "idx 69\n",
            "predict_text 약 100년이\n",
            "\n",
            "\n",
            "idx 70\n",
            "predict_text 암호화폐는\n",
            "\n",
            "\n",
            "idx 71\n",
            "predict_text 정도전에게\n",
            "\n",
            "\n",
            "idx 72\n",
            "predict_text 대륙 철학이라는\n",
            "\n",
            "\n",
            "idx 73\n",
            "predict_text 상수도와 하수도도\n",
            "\n",
            "\n",
            "idx 74\n",
            "predict_text 초등학교 교사로서\n",
            "\n",
            "\n",
            "idx 75\n",
            "predict_text 하페즈 알아사드는\n",
            "\n",
            "\n",
            "idx 76\n",
            "predict_text 아덴만 여명작전에\n",
            "\n",
            "\n",
            "idx 77\n",
            "predict_text 김영대\n",
            "\n",
            "\n",
            "idx 78\n",
            "predict_text 1949년\n",
            "\n",
            "\n",
            "idx 79\n",
            "predict_text 1986년에\n",
            "\n",
            "\n",
            "idx 80\n",
            "predict_text 향토예비군법\n",
            "\n",
            "\n",
            "idx 81\n",
            "predict_text 외젠 드 보아르네에게\n",
            "\n",
            "\n",
            "idx 82\n",
            "predict_text 적절한 비율로\n",
            "\n",
            "\n",
            "idx 83\n",
            "predict_text 사 捨의\n",
            "\n",
            "\n",
            "idx 84\n",
            "predict_text 김대중은\n",
            "\n",
            "\n",
            "idx 85\n",
            "predict_text 입헌군주국이라는\n",
            "\n",
            "\n",
            "idx 86\n",
            "predict_text 북한 잠수함 침투사건\n",
            "\n",
            "\n",
            "idx 87\n",
            "predict_text 해금 정책으로\n",
            "\n",
            "\n",
            "idx 88\n",
            "predict_text 후쿠시마 원전사고이후\n",
            "\n",
            "\n",
            "idx 89\n",
            "predict_text 제국의 아이들의\n",
            "\n",
            "\n",
            "idx 90\n",
            "predict_text 김옥균은\n",
            "\n",
            "\n",
            "idx 91\n",
            "predict_text 미국 육군 부참모 총장과\n",
            "\n",
            "\n",
            "idx 92\n",
            "predict_text 동진사로\n",
            "\n",
            "\n",
            "idx 93\n",
            "predict_text 호남출신인\n",
            "\n",
            "\n",
            "idx 94\n",
            "predict_text 10명을\n",
            "\n",
            "\n",
            "idx 95\n",
            "predict_text 7월 8일\n",
            "\n",
            "\n",
            "idx 96\n",
            "predict_text 죄형법정주의에\n",
            "\n",
            "\n",
            "idx 97\n",
            "predict_text 일본 닌텐도 스페이스 월드 엑스포에서\n",
            "\n",
            "\n",
            "idx 98\n",
            "predict_text 24명의\n",
            "\n",
            "\n",
            "idx 99\n",
            "predict_text 3가지로\n",
            "\n",
            "\n",
            "idx 100\n",
            "predict_text abc방송사에서\n",
            "\n",
            "\n",
            "idx 101\n",
            "predict_text clichè를\n",
            "\n",
            "\n",
            "idx 102\n",
            "predict_text 2014년\n",
            "\n",
            "\n",
            "idx 103\n",
            "predict_text 로베르타 캐슬린 팍스가\n",
            "\n",
            "\n",
            "idx 104\n",
            "predict_text 공무원 급여를\n",
            "\n",
            "\n",
            "idx 105\n",
            "predict_text 국민신당을\n",
            "\n",
            "\n",
            "idx 106\n",
            "predict_text 1958년\n",
            "\n",
            "\n",
            "idx 107\n",
            "predict_text 전국 천하영웅의 시대에\n",
            "\n",
            "\n",
            "idx 108\n",
            "predict_text 체크메이트\n",
            "\n",
            "\n",
            "idx 109\n",
            "predict_text 블루스 페스티벌은\n",
            "\n",
            "\n",
            "idx 110\n",
            "predict_text 구약학\n",
            "\n",
            "\n",
            "idx 111\n",
            "predict_text 빌헬름 마르크스를\n",
            "\n",
            "\n",
            "idx 112\n",
            "predict_text 40명은\n",
            "\n",
            "\n",
            "idx 113\n",
            "predict_text 할아버지의\n",
            "\n",
            "\n",
            "idx 114\n",
            "predict_text 노무현 정권의\n",
            "\n",
            "\n",
            "idx 115\n",
            "predict_text 도올 김용옥은\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j29StLtG5C6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "e353871b-45bc-4507-a6c4-2ccdad3d5500"
      },
      "source": [
        "df = pd.DataFrame(my_answer)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prediction_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>치안관리처벌법에</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2013년</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>빛은</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>타이베리움을</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>11월 3일</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  id prediction_text\n",
              "0  1        치안관리처벌법에\n",
              "1  2           2013년\n",
              "2  3              빛은\n",
              "3  4          타이베리움을\n",
              "4  5          11월 3일"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX7RsLbed6pP"
      },
      "source": [
        "##2.6 후처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1jruy-o1hFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30315982-159e-4ebc-e45a-a27e900aae61"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Hannanum\n",
        "\n",
        "df = pd.DataFrame(my_answer)\n",
        "\n",
        "##마지막 단어가 조사이면 제거한다\n",
        "\n",
        "\n",
        "tokenizer_o = Okt()\n",
        "tokenizer_k = Kkma()\n",
        "tokenizer_h = Hannanum()\n",
        "\n",
        "J_han = [\n",
        "  'J',  # : 관계언 (격조사가 포함)\n",
        "  'X',  #  : 접사 (접두사 접미사 등이 포함)\n",
        "]\n",
        "\n",
        "\n",
        "new_answer = []\n",
        "\n",
        "for sentence in df['prediction_text']:\n",
        "\n",
        "    tmp = tokenizer_o.pos(sentence)\n",
        "    \n",
        "    if tmp[len(tmp)-1][1] in ['Josa'] :\n",
        "       del_word = tmp[len(tmp)-1][0]\n",
        "       del_word = '(.*)'+ del_word +'[^.]*'\n",
        "       sentence = re.sub(del_word, '\\\\1', sentence)  ##마지막 조사를 제외한 문장으로 교체\n",
        "\n",
        "    #tmp = tokenizer_k.pos(sentence)              \n",
        "\n",
        "    #if tmp[len(tmp)-1][1] in J_kkma  :\n",
        "        #del_word = tmp[len(tmp)-1][0]\n",
        "        #del_word = '(.*)'+ del_word +'[^.]*'\n",
        "        #sentence = re.sub(del_word, '\\\\1', sentence) ## Okt로 걸러지지 않는 조사를 Check, 조사를 제외한 문장으로 교체, \n",
        "                                                                     ## kkma는 test 점수 하락이 발생함으로 적용하지 않음\n",
        "\n",
        "    tmp = tokenizer_h.pos(sentence)\n",
        "\n",
        "    if tmp[len(tmp)-1][1] in J_han  :\n",
        "        del_word = tmp[len(tmp)-1][0]\n",
        "        del_word = '(.*)'+ del_word +'[^.]*'\n",
        "        sentence = re.sub(del_word, '\\\\1', sentence) ## 걸러지지 않는 조사를 Check, 조사를 제외한 문장으로 교체\n",
        "\n",
        "    sentence = re.sub(r'[\\u4e00-\\u9fff]+', '', sentence)   ## 한자 제거\n",
        "    sentence = re.sub('\\s+$', '',sentence)                     ## 마지막에 space 있으면 제거\n",
        "\n",
        "    print(sentence)\n",
        "    print(\" \")\n",
        "\n",
        "    new_answer.append(sentence)\n",
        "\n",
        "df['prediction_text'] = new_answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "치안관리처벌법\n",
            " \n",
            "2013년\n",
            " \n",
            "빛\n",
            " \n",
            "타이베리움\n",
            " \n",
            "11월 3일\n",
            " \n",
            "대구 경북지역\n",
            " \n",
            "10\n",
            " \n",
            "save me\n",
            " \n",
            "제3차 하리코프 공방전\n",
            " \n",
            "동대문아파트\n",
            " \n",
            "사라예보\n",
            " \n",
            "경영자\n",
            " \n",
            "자신의 개인 활동\n",
            " \n",
            "다케조에 신이치로 竹添進一郎\n",
            " \n",
            "빛의 제국\n",
            " \n",
            "섭외민사관계 법률적용법 섭외민사관계법률적용법\n",
            " \n",
            "김대중 김종필\n",
            " \n",
            "그랑프리\n",
            " \n",
            "이완\n",
            " \n",
            "22조 9000억원\n",
            " \n",
            "600000장\n",
            " \n",
            "음악적 형태와 장르 재즈는 음악적 예술 형태\n",
            " \n",
            "시카고 블루스\n",
            " \n",
            "세계일보\n",
            " \n",
            "블록체인 blockchain\n",
            " \n",
            "지난해 4월\n",
            " \n",
            "부산\n",
            " \n",
            "30일\n",
            " \n",
            "마스크\n",
            " \n",
            "샤이크 미스킨\n",
            " \n",
            "15년\n",
            " \n",
            "sbs\n",
            " \n",
            "1992년\n",
            " \n",
            "장진\n",
            " \n",
            "요르단 강\n",
            " \n",
            "서울올림픽주경기장\n",
            " \n",
            "이진호 李軫鎬\n",
            " \n",
            "업의 성질\n",
            " \n",
            "정수기 cf\n",
            " \n",
            "차범근 축구교실\n",
            " \n",
            "송파구 장미아파트\n",
            " \n",
            "4월 3일\n",
            " \n",
            "300만 원\n",
            " \n",
            "이회창\n",
            " \n",
            "진상은 이렇다라는\n",
            " \n",
            "평민당사\n",
            " \n",
            "이븐 알나딤 ibn\n",
            " \n",
            "문헌 文憲\n",
            " \n",
            "2007년 12월 7일\n",
            " \n",
            "윤상 콘서트play\n",
            " \n",
            "하프연주\n",
            " \n",
            "대포동 2호\n",
            " \n",
            "인천\n",
            " \n",
            "한겨레\n",
            " \n",
            "2012년 7월 5일\n",
            " \n",
            "화합적취 和合積聚 즉 화합하여 쌓인다는 뜻\n",
            " \n",
            "김홍집\n",
            " \n",
            "환경권\n",
            " \n",
            "우리 집에 왜 왔니\n",
            " \n",
            "seethrough hmd\n",
            " \n",
            "도둑들\n",
            " \n",
            "알렉산드르 1세\n",
            " \n",
            "최동훈\n",
            " \n",
            "1천100억 달러\n",
            " \n",
            "승해 勝解\n",
            " \n",
            "내부 기생충\n",
            " \n",
            "점프 블루스\n",
            " \n",
            "스크린 scrin\n",
            " \n",
            "김구\n",
            " \n",
            "약 100년\n",
            " \n",
            "암호화폐\n",
            " \n",
            "정도전\n",
            " \n",
            "대륙 철학\n",
            " \n",
            "상수도와 하수도\n",
            " \n",
            "초등학교 교사\n",
            " \n",
            "하페즈 알아사드\n",
            " \n",
            "아덴만 여명작전\n",
            " \n",
            "김영대\n",
            " \n",
            "1949년\n",
            " \n",
            "1986년\n",
            " \n",
            "향토예비군법\n",
            " \n",
            "외젠 드 보아르\n",
            " \n",
            "적절한 비율\n",
            " \n",
            "사 捨\n",
            " \n",
            "김대중\n",
            " \n",
            "입헌군주국\n",
            " \n",
            "북한 잠수함 침투사건\n",
            " \n",
            "해금 정책\n",
            " \n",
            "후쿠시마 원전사고이후\n",
            " \n",
            "제국의 아이들\n",
            " \n",
            "김옥균\n",
            " \n",
            "미국 육군 부참모 총장\n",
            " \n",
            "동진사\n",
            " \n",
            "호남출신\n",
            " \n",
            "10명\n",
            " \n",
            "7월 8일\n",
            " \n",
            "죄형법정주의\n",
            " \n",
            "일본 닌텐도 스페이스 월드 엑스포\n",
            " \n",
            "24명\n",
            " \n",
            "3가지\n",
            " \n",
            "abc방송사\n",
            " \n",
            "clichè\n",
            " \n",
            "2014년\n",
            " \n",
            "로베르타 캐슬린 팍스\n",
            " \n",
            "공무원 급여\n",
            " \n",
            "국민신당\n",
            " \n",
            "1958년\n",
            " \n",
            "전국 천하영웅의 시대\n",
            " \n",
            "체크메이트\n",
            " \n",
            "블루스 페스티벌\n",
            " \n",
            "구약학\n",
            " \n",
            "빌헬름 마르크스\n",
            " \n",
            "40명\n",
            " \n",
            "할아버지\n",
            " \n",
            "노무현 정권\n",
            " \n",
            "도올 김용옥\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-9ozw5bBPtb"
      },
      "source": [
        "df.to_csv('my_answer210624_v20_15000.csv', index=False, encoding='utf-8')\n",
        "\n",
        "# v*0은 한자 제거 v*9는 한자 포함으로 설정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iAT-xt4Qyab"
      },
      "source": [
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6J7m_PeNyu1",
        "outputId": "334c5a6b-5e3f-49b9-8af5-031deee717eb"
      },
      "source": [
        "## 아래코드는 각 checkpoint별로 save 파일을 자동생성한다. \n",
        "## 단 colab 내부에 제약에 local pc에서 구동됨\n",
        "\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Hannanum\n",
        "\n",
        "\n",
        "checkpoints = ['9000', '12000', '15000', '18000', '21000', '24000']\n",
        "\n",
        "adr_checkpoints = \"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-\"\n",
        "save_name = 'my_answer20210624_v20_'                     # v*0은 한자 제거 v*9는 한자 포함으로 설정\n",
        "\n",
        "J_kkma = ['JKG' , 'JKM', 'JKS', 'JX', 'JC', 'JKO']  \n",
        "\n",
        "tokenizer_o = Okt()\n",
        "tokenizer_k = Kkma()\n",
        "tokenizer_h = Hannanum()\n",
        "\n",
        "J_han = [\n",
        "  'J',  # : 관계언 (격조사가 포함)\n",
        "  'X',  #  : 접사 (접두사 접미사 등이 포함)\n",
        "]\n",
        "\n",
        "\n",
        "for c_points in checkpoints :\n",
        "\n",
        "    tokenizer = ElectraTokenizer.from_pretrained(adr_checkpoints + c_points)\n",
        "    model = ElectraForQuestionAnswering.from_pretrained(adr_checkpoints + c_points)\n",
        "    prediction_qa = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
        "\n",
        "    my_answer = {}\n",
        "    my_answer['id']=[]\n",
        "    my_answer['prediction_text']=[]\n",
        "\n",
        "    for idx, sample in enumerate(test_json['data']):\n",
        "\n",
        "        if idx%10==0 :\n",
        "            print(c_points, idx)\n",
        "\n",
        "        context = sample['context']\n",
        "        question = sample['question']\n",
        "        answer_dict = prediction_qa({'question':question,'context': context})\n",
        "        predict_text = normalize_answer_mod(answer_dict[\"answer\"])\n",
        "    \n",
        "#       print(f'context {context}')\n",
        "#       print(f'question {question}')\n",
        "#       print(f'predict_text {predict_text}')\n",
        "#       print('\\n')\n",
        "    \n",
        "        my_answer['id'].append(str(idx + 1))\n",
        "        my_answer['prediction_text'].append(predict_text)\n",
        "\n",
        "    df = pd.DataFrame( my_answer )\n",
        "\n",
        "\n",
        "    new_answer = []\n",
        "\n",
        "    for sentence in df['prediction_text']:\n",
        "\n",
        "        tmp = tokenizer_o.pos(sentence)\n",
        "    \n",
        "        if tmp[len(tmp)-1][1] in ['Josa'] :\n",
        "            del_word = tmp[len(tmp)-1][0]\n",
        "            del_word = '(.*)'+ del_word +'[^.]*'\n",
        "            sentence = re.sub(del_word, '\\\\1', sentence)  ##마지막 조사를 제외한 문장으로 교체\n",
        "\n",
        "        #tmp = tokenizer_k.pos(sentence)\n",
        "\n",
        "        #if tmp[len(tmp)-1][1] in J_kkma  :\n",
        "            #del_word = tmp[len(tmp)-1][0]\n",
        "            #del_word = '(.*)'+ del_word +'[^.]*'\n",
        "            #sentence = re.sub(del_word, '\\\\1', sentence) ## Okt로 걸러지지 않는 조사를 Check, 조사를 제외한 문장으로 교체\n",
        "\n",
        "        tmp = tokenizer_h.pos(sentence)\n",
        "\n",
        "        if tmp[len(tmp)-1][1] in J_han  :\n",
        "            del_word = tmp[len(tmp)-1][0]\n",
        "            del_word = '(.*)'+ del_word +'[^.]*'\n",
        "            sentence = re.sub(del_word, '\\\\1', sentence) ## 걸러지지 않는 조사를 Check, 조사를 제외한 문장으로 교체\n",
        "\n",
        "        sentence = re.sub(r'[\\u4e00-\\u9fff]+', '', sentence)   ## 한자 제거\n",
        "        sentence = re.sub('\\s+$', '',sentence)                     ## 마지막에 space 있으면 제거\n",
        "        \n",
        "        new_answer.append(sentence)\n",
        "\n",
        "\n",
        "    df['prediction_text'] = new_answer\n",
        "\n",
        "    df.to_csv(save_name+c_points+'.csv', index=False, encoding='utf-8')\n",
        "    print(f'{c_points} saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9000 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}